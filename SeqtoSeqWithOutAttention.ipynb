{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11817792,"sourceType":"datasetVersion","datasetId":7422871},{"sourceId":11875800,"sourceType":"datasetVersion","datasetId":7463561}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import Lib\n\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.autograd import Variable \nimport copy\nimport os\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nfrom torch.utils.data import DataLoader, TensorDataset\nimport random\nimport heapq\nimport wandb\n# Set device (CUDA if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-05-20T07:25:19.497873Z","iopub.execute_input":"2025-05-20T07:25:19.498310Z","iopub.status.idle":"2025-05-20T07:25:26.736811Z","shell.execute_reply.started":"2025-05-20T07:25:19.498274Z","shell.execute_reply":"2025-05-20T07:25:26.736190Z"},"trusted":true},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!wget https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\n!tar -xf dakshina_dataset_v1.0.tar","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T20:30:33.069992Z","iopub.execute_input":"2025-05-19T20:30:33.070708Z","iopub.status.idle":"2025-05-19T20:30:42.836599Z","shell.execute_reply.started":"2025-05-19T20:30:33.070685Z","shell.execute_reply":"2025-05-19T20:30:42.835487Z"}},"outputs":[{"name":"stdout","text":"--2025-05-19 20:30:33--  https://storage.googleapis.com/gresearch/dakshina/dakshina_dataset_v1.0.tar\nResolving storage.googleapis.com (storage.googleapis.com)... 173.194.203.207, 74.125.199.207, 172.253.117.207, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|173.194.203.207|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2008340480 (1.9G) [application/x-tar]\nSaving to: ‘dakshina_dataset_v1.0.tar’\n\ndakshina_dataset_v1 100%[===================>]   1.87G   267MB/s    in 7.3s    \n\n2025-05-19 20:30:40 (262 MB/s) - ‘dakshina_dataset_v1.0.tar’ saved [2008340480/2008340480]\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"wandb.login(key='b8d44a4abbab8753e976a6e5ab717fd669ba99a2')\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T07:25:30.872111Z","iopub.execute_input":"2025-05-20T07:25:30.872561Z","iopub.status.idle":"2025-05-20T07:25:37.447436Z","shell.execute_reply.started":"2025-05-20T07:25:30.872535Z","shell.execute_reply":"2025-05-20T07:25:37.446690Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m030\u001b[0m (\u001b[33mcs24m030-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"def load_and_convert_dataset(tsv_file_path, csv_output_path=None):\n    \"\"\"\n    Load a TSV dataset and convert it to CSV format\n    \n    Args:\n        tsv_file_path: Path to the TSV file\n        csv_output_path: Path to save the CSV file (if None, will use same name with .csv extension)\n    \n    Returns:\n        df: Pandas DataFrame containing the dataset\n        input_len: Maximum length of input sequences\n        output_len: Maximum length of output sequences\n    \"\"\"\n    # Load TSV file\n    df = pd.read_csv(tsv_file_path, sep='\\t', header=None)\n    \n    # Assuming the first column is the input and the second column is the output\n    if len(df.columns) >= 2:\n        df.columns = ['input', 'output'] + [f'col_{i}' for i in range(2, len(df.columns))]\n        \n        # Calculate length statistics\n        input_len = df['input'].str.len().max()\n        output_len = df['output'].str.len().max()\n        \n        # Save as CSV if requested\n        if csv_output_path is None:\n            csv_output_path = tsv_file_path.replace('.tsv', '.csv')\n            \n        df.to_csv(csv_output_path, index=False)\n        print(f\"Converted {tsv_file_path} to {csv_output_path}\")\n        \n        return df, input_len, output_len\n    else:\n        print(f\"Error: The TSV file {tsv_file_path} doesn't have at least 2 columns\")\n        return None, None, None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-20T07:25:39.652607Z","iopub.execute_input":"2025-05-20T07:25:39.653339Z","iopub.status.idle":"2025-05-20T07:25:39.660062Z","shell.execute_reply.started":"2025-05-20T07:25:39.653299Z","shell.execute_reply":"2025-05-20T07:25:39.658982Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Preprocessing","metadata":{}},{"cell_type":"code","source":"def encode(x, max_length, char_to_idx):\n    \"\"\"\n    Encode a string into a tensor.\n\n    Args:\n    - x (str): Input string to encode.\n    - max_length (int): Maximum length for the encoded tensor.\n    - char_to_idx (dict): Character to index mapping.\n\n    Returns:\n    - encoded (torch.Tensor): Encoded tensor.\n    - length (int): Actual length of the encoded sequence.\n    \"\"\"\n    encoded = np.zeros(max_length, dtype=int)\n    encoder = np.array([char_to_idx[char] for char in x])\n    length = min(max_length, len(encoder))\n    encoded[:length] = encoder[:length]\n\n    return torch.tensor(encoded, dtype=torch.int64), length\n\ndef get_tensor_object(df, max_input_length, max_output_length, char_to_idx_input, char_to_idx_output):\n    \"\"\"\n    Create tensor objects from a DataFrame.\n\n    Args:\n    - df (pd.DataFrame): Input DataFrame containing input and output sequences.\n    - max_input_length (int): Maximum length for input sequences.\n    - max_output_length (int): Maximum length for output sequences.\n    - char_to_idx_input (dict): Character to index mapping for input sequences.\n    - char_to_idx_output (dict): Character to index mapping for output sequences.\n\n    Returns:\n    - tensor_inputs (torch.Tensor): Tensor containing encoded input sequences.\n    - tensor_outputs (torch.Tensor): Tensor containing encoded output sequences.\n    \"\"\"\n    \n    # Encode unique inputs and outputs into tensors\n    encoded_inputs = []\n    encoded_outputs = []\n\n    # Encode the input column\n    for input_str in df[0]:\n        encoded_input, input_length = encode(input_str, max_input_length, char_to_idx_input)\n        encoded_inputs.append(encoded_input)\n\n    # Encode the output column\n    for output_str in df[1]:\n        encoded_output, output_length = encode(output_str, max_output_length, char_to_idx_output)\n        encoded_outputs.append(encoded_output)\n\n    # Stack tensors column-wise\n    \n#     tensor_inputs = torch.stack(encoded_inputs, dim=1)\n#     tensor_outputs = torch.stack(encoded_outputs, dim=1)\n    tensor_inputs = torch.stack(encoded_inputs)\n    tensor_outputs = torch.stack(encoded_outputs)\n\n    return tensor_inputs, tensor_outputs\n\ndef load_dataset(path):\n    \"\"\"\n    Load a dataset from a TSV file.\n    Args:\n    - path (str): Path to the TSV file.\n    Returns:\n    - df (pd.DataFrame): Loaded DataFrame.\n    - max_input_length (int): Maximum length for input sequences.\n    - max_output_length (int): Maximum length for output sequences.\n    \"\"\"\n    df = pd.read_csv(path, header=None, encoding='utf-8', sep='\\t')  # Changed separator to tab\n    \n    # Convert values to strings before adding special characters\n    df[0] = df[0].astype(str).apply(lambda x: x + '$')\n    df[1] = df[1].astype(str).apply(lambda x: '^' + x + '$')\n    \n    # Determine maximum length for input and output sequences\n    max_input_length = max(len(x) for x in df[0].unique())\n    max_output_length = max(len(x) for x in df[1].unique())\n    return df, max_input_length, max_output_length\n\ndef look_up_table(vocab1, vocab2, vocab3):\n    \"\"\"\n    Create lookup tables for vocabulary mapping.\n\n    Args:\n    - vocab1 (list): First list of vocabulary items.\n    - vocab2 (list): Second list of vocabulary items.\n    - vocab3 (list): Third list of vocabulary items.\n\n    Returns:\n    - vocab_to_int (dict): Mapping from vocabulary items to integers.\n    - int_to_vocab (dict): Mapping from integers to vocabulary items.\n    \"\"\"\n    \n    # Combine all vocabularies into one set\n    vocab = set(''.join(vocab1) + ''.join(vocab2) + ''.join(vocab3))\n    vocab.discard('^')  \n    vocab.discard('$')  \n    vocab_to_int = {\"\": 0, '^':1, '$':2}\n    for v_i, v in enumerate(sorted(vocab), len(vocab_to_int)):\n        vocab_to_int[v] = v_i\n    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}\n    return vocab_to_int, int_to_vocab\n\n\n\n\n# # Load Train, Val, Test\n# df_train, train_input_len, train_out_len = load_dataset('/kaggle/input/dataset/aksharantar_sampled/hin/hin_train.csv')\n# df_val, val_input_len, val_out_len = load_dataset('/kaggle/input/dataset/aksharantar_sampled/hin/hin_valid.csv')\n# df_test, test_input_len, test_out_len = load_dataset('/kaggle/input/dataset/aksharantar_sampled/hin/hin_test.csv')\n\n# input_max_len = max(train_input_len, val_input_len, test_input_len)\n# output_max_len = max(train_out_len, val_out_len, test_out_len)\n\n\n# # Create Look Up Table\n# input_char_to_int, input_int_to_char = look_up_table(df_train[0], df_val[0], df_test[0])\n# output_char_to_int, output_int_to_char = look_up_table(df_train[1], df_val[1], df_test[1])\n\n# print(\"Input Lookup Table:\", input_char_to_int)\n# print(\"\\n\\n Output Lookup Table\", output_char_to_int)\n\n# # Data Embedding and Converting them into Tensor\n# train_inputs, train_outputs = get_tensor_object(df_train, input_max_len, input_max_len, input_char_to_int, output_char_to_int)\n# val_inputs, val_outputs = get_tensor_object(df_val, input_max_len, input_max_len, input_char_to_int, output_char_to_int)\n# test_inputs, test_outputs = get_tensor_object(df_test, input_max_len, input_max_len, input_char_to_int, output_char_to_int)\n\n# # Transpose column wise\n# train_inputs, train_outputs = torch.transpose(train_inputs, 0, 1), torch.transpose(train_outputs, 0, 1)\n# val_inputs, val_outputs = torch.transpose(val_inputs, 0, 1), torch.transpose(val_outputs, 0, 1)\n# test_inputs, test_outputs = torch.transpose(test_inputs, 0, 1), torch.transpose(test_outputs, 0, 1)\n\n\n# print(\"\\n\", train_inputs[:,0],train_outputs[:,0])\n# print(\"Training:\", train_inputs.shape, train_outputs.shape)\n\n# print(\"Validation\", val_inputs.shape, val_inputs.shape)\n# print(df_train.head())","metadata":{"execution":{"iopub.status.busy":"2025-05-20T07:25:44.009500Z","iopub.execute_input":"2025-05-20T07:25:44.009744Z","iopub.status.idle":"2025-05-20T07:25:44.022300Z","shell.execute_reply.started":"2025-05-20T07:25:44.009726Z","shell.execute_reply":"2025-05-20T07:25:44.021541Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Create Seq2Seq Model","metadata":{}},{"cell_type":"markdown","source":"## encoder and decoder","metadata":{}},{"cell_type":"code","source":"class Encoder(nn.Module): \n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout, bidirectional, cell_type):\n        super(Encoder, self).__init__()\n        self.bidirectional = bidirectional\n        self.dropout = nn.Dropout(dropout)\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.cell_type = cell_type\n        \n        # Define embedding layer\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        \n        # Define RNN layer with specific cell type\n        if cell_type == 'LSTM':\n            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=bidirectional)\n        elif cell_type == 'GRU':\n            self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=bidirectional)\n        elif cell_type == 'RNN':\n            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=bidirectional)\n        else:\n            raise ValueError(\"Invalid RNN type. Choose from 'LSTM', 'GRU', or 'RNN'.\")\n        \n        \n    def forward(self, x): # x shape: (seq_length, N) where N is batch size\n        # Perform dropout on the input\n        embedding = self.embedding(x)\n        embedding = self.dropout(embedding) # embedding shape: (seq_length, N, embedding_size)\n        \n        if self.cell_type == \"LSTM\":\n            # Pass through the LSTM layer\n            outputs, (hidden, cell) = self.rnn(embedding) # outputs shape: (seq_length, N, hidden_size)\n            if self.bidirectional:\n                # Sum the bidirectional outputs\n                outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n                hidden = torch.cat((hidden[: self.num_layers], hidden[self.num_layers:]), dim=0)\n            # Return hidden state and cell state   \n            return hidden, cell\n        elif self.cell_type == \"GRU\" or self.cell_type == \"RNN\":\n            # Pass through the RNN/GRU layer\n            outputs, hidden = self.rnn(embedding) # outputs shape: (seq_length, N, hidden_size)\n            if self.bidirectional:\n                # Sum the bidirectional outputs\n                outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n                hidden = torch.cat((hidden[: self.num_layers], hidden[self.num_layers:]), dim=0)\n\n            # Return hidden state and cell state\n            return hidden\n        else:\n            print(\"Invalid cell_type specified for Encoder.\")\n            return None\n\n\nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, dropout, bidirectional, cell_type):\n        super(Decoder, self).__init__()\n        self.bidirectional = bidirectional\n        self.dropout = nn.Dropout(dropout)  \n        self.num_layers = num_layers \n        self.hidden_size = hidden_size\n        self.embedding_size = embedding_size\n        self.cell_type = cell_type\n        \n        # Define embedding layer\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        \n        # Define RNN layer with specific cell type\n        if cell_type == 'LSTM':\n            self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=bidirectional)\n        elif cell_type == 'GRU':\n            self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=bidirectional)\n        elif cell_type == 'RNN':\n            self.rnn = nn.RNN(embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=bidirectional)\n        else:\n            raise ValueError(\"Invalid RNN type. Choose from 'LSTM', 'GRU', or 'RNN'.\")\n            \n            \n        # Define fully connected layer\n        self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, output_size)  # Adjust input size for bidirectional decoder\n        # Softmax layer\n        self.log_softmax = nn.LogSoftmax(dim=1)\n    \n    def forward(self, x, hidden, cell): # x shape: (N) where N is for batch size, we want it to be (1, N), seq_length\n        \n        # Ensure x has the shape (1, N)\n        x = x.unsqueeze(0)\n        \n        # Perform dropout on the input\n        embedding = self.embedding(x)\n        embedding = self.dropout(embedding)  # embedding shape: (1, N, embedding_size)\n        \n        if self.cell_type == \"LSTM\":\n            # Pass through the LSTM layer\n            outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))  # outputs shape: (1, N, hidden_size * num_directions)\n\n            # Pass through fully connected layer\n            out = self.fc(outputs).squeeze(0)\n            predictions = self.log_softmax(out)\n\n            return predictions, hidden, cell\n        elif self.cell_type == \"GRU\" or self.cell_type == \"RNN\":\n            # Pass through the RNN/GRU layer\n            outputs, hidden = self.rnn(embedding, hidden)  # outputs shape: (1, N, hidden_size * num_directions)\n\n            # Pass through fully connected layer\n            out = self.fc(outputs).squeeze(0)\n            predictions = self.log_softmax(out)\n\n            return predictions, hidden\n\n        else:\n            print(\"Invalid cell_type specified for Decoder.\")\n            return None\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T07:25:49.610133Z","iopub.execute_input":"2025-05-20T07:25:49.610417Z","iopub.status.idle":"2025-05-20T07:25:49.623154Z","shell.execute_reply.started":"2025-05-20T07:25:49.610395Z","shell.execute_reply":"2025-05-20T07:25:49.622461Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Seq2Seq Class","metadata":{}},{"cell_type":"code","source":"class Seq2Seq(nn.Module):\n\n    def __init__(self, encoder, decoder, output_char_to_int, teacher_forcing, cell_type):\n\n        super(Seq2Seq, self).__init__()  \n        # Initialize encoder and decoder\n        self.decoder = decoder\n        self.encoder = encoder\n        self.cell_type = cell_type\n        self.target_vocab_size = len(output_char_to_int)\n        self.teacher_force_ratio = teacher_forcing\n        \n    def forward(self, source, target):\n        # Get batch size, target length, and target vocabulary size\n        batch_size = source.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = self.target_vocab_size\n        teacher_force_ratio = self.teacher_force_ratio\n        \n        # Initialize outputs tensor\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(source.device)\n        # Grab the first input to the Decoder which will be <SOS> token i.e '^'\n        x = target[0]\n        # Get hidden state and cell state from encoder\n        if self.cell_type == 'LSTM':\n            hidden, cell = self.encoder(source)\n        else:\n            hidden = self.encoder(source)\n        \n        for t in range(1, target_len):\n            # Use previous hidden and cell states as context from encoder at start\n            if self.cell_type == 'LSTM':\n                output, hidden, cell = self.decoder(x, hidden, cell)\n            else:\n                output, hidden = self.decoder(x, hidden, None)\n                \n            # Store next output prediction\n            outputs[t] = output\n            # Get the best word the Decoder predicted (index in the vocabulary)\n            best_guess = output.argmax(1)\n            # Update input for next time step based on teacher forcing ratio\n            x = best_guess if random.random() >= teacher_force_ratio else target[t]\n\n        return outputs","metadata":{"execution":{"iopub.status.busy":"2025-05-20T07:25:55.552705Z","iopub.execute_input":"2025-05-20T07:25:55.553168Z","iopub.status.idle":"2025-05-20T07:25:55.559560Z","shell.execute_reply.started":"2025-05-20T07:25:55.553144Z","shell.execute_reply":"2025-05-20T07:25:55.558785Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# TRAINING","metadata":{}},{"cell_type":"code","source":"# BEAM SEARCH FUNCTION\ndef beam_search(model, input_seq, max_length, input_char_index, output_char_index, reverse_target_char_index, beam_width, length_penalty, cell_type):\n    \"\"\"\n    Perform beam search to generate a sequence using the provided model.\n\n    Args:\n    - model (nn.Module): The Seq2Seq model.\n    - input_seq (str): The input sequence.\n    - max_length (int): Maximum length of the input sequence.\n    - input_char_index (dict): Mapping from characters to integers for the input vocabulary.\n    - output_char_index (dict): Mapping from characters to integers for the output vocabulary.\n    - reverse_target_char_index (dict): Reverse mapping from integers to characters for the output vocabulary.\n    - beam_width (int): Beam width for beam search.\n    - length_penalty (float): Length penalty for beam search.\n    - cell_type (str): Type of RNN cell used in the model ('LSTM', 'GRU', or 'RNN').\n\n    Returns:\n    - str: The generated output sequence.\n    \"\"\"\n    if len(input_seq) > max_length:\n        print(\"Input Length is exceeding max length!!!!\")\n        return \"\"\n\n    # Create np array of zeros of length input\n    input_data = np.zeros((max_length, 1), dtype=int)  # (N,1)\n\n    # Encode the input\n    for idx, char in enumerate(input_seq):\n        input_data[idx, 0] = input_char_index[char]\n    input_data[idx + 1, 0] = input_char_index[\"$\"]  # EOS\n\n    # Convert to tensor\n    input_tensor = torch.tensor(input_data, dtype=torch.int64).to(device)  # N,1\n\n    with torch.no_grad():\n        if cell_type == 'LSTM':\n            hidden, cell = model.encoder(input_tensor)\n\n        else:\n            hidden = model.encoder(input_tensor)\n\n    # Initialize beam\n    out_t = output_char_index['^']\n    out_reshape = np.array(out_t).reshape(1,)\n    hidden_par = hidden.unsqueeze(0)\n    initial_sequence = torch.tensor(out_reshape).to(device)\n    beam = [(0.0, initial_sequence, hidden_par)]  # [(score, sequence, hidden)]\n\n    for _ in range(len(output_char_index)):\n        candidates = []\n        for score, seq, hidden in beam:\n            if seq[-1].item() == output_char_index['$']:\n                # If the sequence ends with the end token, add it to the candidates\n                candidates.append((score, seq, hidden))\n                continue\n\n            last_token = np.array(seq[-1].item()).reshape(1,)\n            x = torch.tensor(last_token).to(device)\n\n            if cell_type == 'LSTM':\n                output, hidden, cell,  = model.decoder(x, hidden.squeeze(0), cell)\n            else:\n                output, hidden,  = model.decoder(x, hidden.squeeze(0), None)\n\n            probabilities = F.softmax(output, dim=1)\n            topk_probs, topk_tokens = torch.topk(probabilities, k=beam_width)\n\n            for prob, token in zip(topk_probs[0], topk_tokens[0]):\n                new_seq = torch.cat((seq, token.unsqueeze(0)), dim=0)\n                seq_length_norm_factor = (len(new_seq) - 1) / 5\n                candidate_score = score + torch.log(prob).item() / (seq_length_norm_factor ** length_penalty)\n                candidates.append((candidate_score, new_seq, hidden.unsqueeze(0)))\n\n        # Select top-k candidates based on the accumulated scores\n        beam = heapq.nlargest(beam_width, candidates, key=lambda x: x[0])\n\n    best_score, best_sequence, _ = max(beam, key=lambda x: x[0])  # Select the best sequence from the beam as the output\n\n    # Convert the best sequence indices to characters\n    return ''.join([reverse_target_char_index[token.item()] for token in best_sequence[1:]])\n\n\n# TRAINING FUNCTION\ndef train(model, num_epochs, criterion, optimizer, train_batch_x, train_batch_y, val_batch_x, val_batch_y, df_val, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, cell_type, max_length, wandb_log):\n    \"\"\"\n    Train the Seq2Seq model.\n\n    Args:\n    - model (nn.Module): The Seq2Seq model.\n    - num_epochs (int): Number of training epochs.\n    - criterion: Loss criterion for training.\n    - optimizer: Optimizer for training.\n    - train_batch_x: Training input data.\n    - train_batch_y: Training target data.\n    - val_batch_x: Validation input data.\n    - val_batch_y: Validation target data.\n    - df_val: DataFrame for validation data.\n    - input_char_to_int (dict): Mapping from characters to integers for the input vocabulary.\n    - output_char_to_int (dict): Mapping from characters to integers for the output vocabulary.\n    - output_int_to_char (dict): Reverse mapping from integers to characters for the output vocabulary.\n    - beam_width (int): Beam width for beam search.\n    - length_penalty (float): Length penalty for beam search.\n    - cell_type (str): Type of RNN cell used in the model ('LSTM', 'GRU', or 'RNN').\n    - max_length (int): Maximum length of sequences.\n    - wandb_log (int): Whether to log to wandb (1 or 0).\n    Returns:\n    - nn.Module: The trained model.\n    - float: Validation accuracy.\n    \"\"\"\n    for epoch in range(num_epochs):\n        total_words = 0\n        correct_pred = 0\n        total_loss = 0\n        accuracy = 0\n        model.train()\n        \n        # Use tqdm for progress tracking\n        train_data_iterator = tqdm(zip(train_batch_x, train_batch_y), total=len(train_batch_x))\n        \n        for (x, y) in train_data_iterator:\n            # Get input and targets and move to device\n            target, inp_data = y.to(device), x.to(device)\n            \n            # Forward propagation\n            optimizer.zero_grad()\n            output = model(inp_data, target)\n            \n            target = target.reshape(-1)\n            output = output.reshape(-1, output.shape[2])\n            \n            pad_mask = (target != 0)  \n            target = target[pad_mask] # Select non-padding elements\n            output = output[pad_mask] \n            \n            # Calculate loss\n            loss = criterion(output, target)\n            \n            # Backpropagation\n            loss.backward()\n            \n            # Clip gradients to avoid exploding gradients\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n            \n            # Update parameters\n            optimizer.step()\n            \n            # Accumulate total loss\n            total_loss += loss.item()\n            # Update total words processed\n            total_words += target.size(0)\n            # Calculate number of correct predictions\n            correct_pred += torch.sum(torch.argmax(output, dim=1) == target).item()\n            \n        # Calculate average loss per batch\n        avg_loss = total_loss / len(train_batch_x)\n        # Calculate accuracy\n        accuracy = 100*correct_pred / total_words\n        \n        \n        # Validation\n        model.eval()\n        with torch.no_grad():\n            val_total_loss = 0\n            val_total_words = 0\n            val_correct_pred = 0\n\n            val_data_iterator = tqdm(zip(val_batch_x, val_batch_y), total=len(val_batch_x))\n            for x_val, y_val in val_data_iterator:\n                target_val, inp_data_val = y_val.to(device), x_val.to(device)\n                output_val = model(inp_data_val, target_val)\n                \n                \n                target_val = target_val.reshape(-1)\n                output_val = output_val.reshape(-1, output_val.shape[2])\n                \n                pad_mask = (target_val != 0)  \n                target_val = target_val[pad_mask] # Select non-padding elements\n                output_val = output_val[pad_mask] \n            \n                val_loss = criterion(output_val, target_val)\n                val_total_loss += val_loss.item()\n                val_total_words += target_val.size(0)\n                val_correct_pred += torch.sum(torch.argmax(output_val, dim=1) == target_val).item()\n\n            # Calculate validation statistics\n            val_accuracy = 100*val_correct_pred / val_total_words\n            val_avg_loss = val_total_loss / len(val_batch_x)\n\n            \n            \n        # Total word predict correct over training\n        beam_val_pred = 0\n        beam_val = 0\n        for i in tqdm(range(df_val.shape[0])):\n            input_seq = df_val.iloc[i, 0][:-1] \n            true_seq = df_val.iloc[i, 1][1:-1]\n            predicted_output = beam_search(model, input_seq, max_length, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, cell_type)\n            if true_seq == predicted_output[:-1]:\n                beam_val_pred+=1\n        beam_val = 100*beam_val_pred/df_val.shape[0]\n\n\n\n        # Print statistics\n        print(f\"Epoch {epoch + 1} / {num_epochs} ===========================>\")\n        print(f\"Train Accuracy Char: {accuracy:.4f}, Train Average Loss: {avg_loss:.4f}\")\n        print(f\"Validation Accuracy Char: {val_accuracy:.4f}, Validation Average Loss: {val_avg_loss:.4f}\")\n        print(f\"Beam Val Word Accuracy: {beam_val:.4f} Correct Prediction : {beam_val_pred}/{df_val.shape[0]}\")    \n        \n        if wandb_log == 1:\n            wandb.log({\n                \"train_accuracy_char\": accuracy,\n                \"train_loss\": avg_loss,\n                \"val_accuracy_char\": val_accuracy,\n                \"val_loss\": val_avg_loss,\n                \"beam_val_accuracy_word\" : beam_val,\n            })\n        \n    \n    return model, beam_val\n","metadata":{"execution":{"iopub.status.busy":"2025-05-20T07:25:58.655453Z","iopub.execute_input":"2025-05-20T07:25:58.656056Z","iopub.status.idle":"2025-05-20T07:25:58.674176Z","shell.execute_reply.started":"2025-05-20T07:25:58.656030Z","shell.execute_reply":"2025-05-20T07:25:58.673453Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## SWEEP CONFIGURATION","metadata":{}},{"cell_type":"code","source":"def main():\n    wandb.init(project='DA6401_A3')\n    config = wandb.config\n    wandb.run.name = 'cell_' + config.cell_type + '_bs_' + str(config.batch_size) + '_ep_' + str(config.num_epochs) + '_op_' + str(config.optimizer) + '_drop_' + str(config.dropout) + '_bsw_' + str(config.beam_search_width) +'_emb_' + str(config.embedding_size) + '_hs_' + str(config.hidden_size) + '_elayer_' + str(config.num_layers) + '_dlayer_' + str(config.num_layers)\n    \n    # Load Dataset\n    # df_train, train_input_len, train_out_len = load_dataset('/kaggle/input/hinid-dataset/aksharantar_sampled/hin/hin_train.csv')\n    # df_val, val_input_len, val_out_len = load_dataset('/kaggle/input/hinid-dataset/aksharantar_sampled/hin/hin_valid.csv')\n    # df_test, test_input_len, test_out_len = load_dataset('/kaggle/input/hinid-dataset/aksharantar_sampled/hin/hin_test.csv')\n\n    df_train, train_input_len, train_out_len = load_dataset('/kaggle/input/dakshina/hi.translit.sampled.train.tsv')\n    df_val, val_input_len, val_out_len = load_dataset('/kaggle/input/dakshina/hi.translit.sampled.dev.tsv')\n    df_test, test_input_len, test_out_len = load_dataset('/kaggle/input/dakshina/hi.translit.sampled.test.tsv')\n    \n    input_max_len = max(train_input_len, val_input_len, test_input_len)\n    output_max_len = max(train_out_len, val_out_len, test_out_len)\n    \n    max_length = max(input_max_len, output_max_len)\n\n    # Create Look Up Table\n    input_char_to_int, input_int_to_char = look_up_table(df_train[0], df_val[0], df_test[0])\n    output_char_to_int, output_int_to_char = look_up_table(df_train[1], df_val[1], df_test[1])\n\n    # Data Embedding and Converting them into Tensor\n    train_inputs, train_outputs = get_tensor_object(df_train, max_length, max_length, input_char_to_int, output_char_to_int)\n    val_inputs, val_outputs = get_tensor_object(df_val, max_length, max_length, input_char_to_int, output_char_to_int)\n    test_inputs, test_outputs = get_tensor_object(df_test, max_length, max_length, input_char_to_int, output_char_to_int)\n\n    # Transpose column wise\n    train_inputs, train_outputs = torch.transpose(train_inputs, 0, 1), torch.transpose(train_outputs, 0, 1)\n    val_inputs, val_outputs = torch.transpose(val_inputs, 0, 1), torch.transpose(val_outputs, 0, 1)\n    test_inputs, test_outputs = torch.transpose(test_inputs, 0, 1), torch.transpose(test_outputs, 0, 1)\n\n\n    # Initialize Hyperparameters\n    input_size = len(input_char_to_int)\n    output_size = len(output_char_to_int)\n    embedding_size = config.embedding_size\n    hidden_size = config.hidden_size\n    enc_num_layers = config.num_layers\n    dec_num_layers = config.num_layers\n    cell_type = config.cell_type\n    dropout = config.dropout\n    learning_rate = config.learning_rate\n    batch_size = config.batch_size\n    num_epochs = config.num_epochs  \n    optimizer = config.optimizer  \n    beam_width = config.beam_search_width\n    bidirectional = config.bidirectional\n    length_penalty = config.length_penalty\n    teacher_forcing = config.teacher_forcing\n    learning_rate = config.learning_rate\n\n    # Create train data batch\n    train_batch_x, train_batch_y = torch.split(train_inputs, batch_size, dim=1), torch.split(train_outputs, batch_size, dim=1)\n    # Validation data batch\n    val_batch_x, val_batch_y = torch.split(val_inputs, batch_size, dim=1), torch.split(val_outputs, batch_size, dim=1)\n\n\n    # Intialize encoder, decoder and seq2seq model\n    encoder = Encoder(input_size, embedding_size, hidden_size, enc_num_layers, dropout, bidirectional, cell_type).to(device)\n    decoder = Decoder(output_size, embedding_size, hidden_size, output_size, dec_num_layers, dropout, bidirectional, cell_type).to(device)  \n    model = Seq2Seq(encoder, decoder, output_char_to_int, teacher_forcing, cell_type).to(device)\n\n    # Print total number of parameters in the model\n    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(model)\n    print(f'Total Trainable Parameters: {total_params}')\n\n\n    # Loss function and Optimizer\n    criterion = nn.CrossEntropyLoss()\n    if optimizer == 'adam':\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    elif optimizer == 'sgd':\n        optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n    elif optimizer == 'rmsprop':\n        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n    elif optimizer == 'nadam':\n        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n    elif optimizer == 'adagrad':\n        optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n    else:\n        print(\"Incorrect Optmizer !!!!\")\n\n    # TRAINING\n    model, acc = train(model, num_epochs, criterion, optimizer, train_batch_x, train_batch_y, val_batch_x, val_batch_y, df_val, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, cell_type, max_length, 1)\n    wandb.log({\n            \"accuracy\": acc,\n        })\n    \n# SWEEP CONFIG\n# sweep_config = {\n#     'name': 'sweep_2',\n#     'method': 'bayes',  \n#     'metric': {'name': 'accuracy', 'goal': 'maximize'},\n#     'parameters': {\n#         'embedding_size': {'values': [64, 256, 512]},  \n#         'hidden_size': {'values': [256, 512, 1024]},\n#         'num_layers': {'values': [1, 2]},  \n#         'cell_type': {'values':['LSTM', \"GRU\", \"RNN\"]}, # RNN, LSTM, GRU\n#         'dropout': {'values': [0.2, 0.3]},\n#         'learning_rate': {'values': [0.01, 0.001]},\n#         'batch_size': {'values': [ 64,128,256]},\n#         'num_epochs': {'values': [5,10]},\n#         'optimizer': {'values': ['adagrad']}, # ['sgd', 'rmsprop', 'adam', 'nadam']\n#         'beam_search_width': {'values': [1, 3, 5]},\n#         'length_penalty' : {'values': [0.6]},\n#         'bidirectional': {'values': [True]},\n#         'teacher_forcing': {'values': [0.5, 0.7]}\n#     }\n# }\nsweep_config = {\n    'name': 'sweep_2',\n    'method': 'bayes',  \n    'metric': {'name': 'accuracy', 'goal': 'maximize'},\n    'parameters': {\n        'embedding_size': {'values': [512]},  \n        'hidden_size': {'values': [512]},\n        'num_layers': {'values': [1,2]},  \n        'cell_type': {'values':[\"GRU\", \"LSTM\"]}, # RNN, LSTM, GRU\n        'dropout': {'values': [ 0.3,0.5]},\n        'learning_rate': {'values': [0.01]},\n        'batch_size': {'values': [ 32,64]},\n        'num_epochs': {'values': [10]},\n        'optimizer': {'values': ['adagrad']}, # ['sgd', 'rmsprop', 'adam', 'nadam']\n        'beam_search_width': {'values': [1]},\n        'length_penalty' : {'values': [0.6]},\n        'bidirectional': {'values': [True]},\n        'teacher_forcing': {'values': [0.7]}\n    }\n}\n\n\n# RUN SWEEP ID with agent\nsweep_id = wandb.sweep(sweep_config, project = 'DA6401_A3')\nwandb.agent(sweep_id, main, count = 7)\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2025-05-20T05:27:55.272721Z","iopub.execute_input":"2025-05-20T05:27:55.273448Z","iopub.status.idle":"2025-05-20T07:08:49.749302Z","shell.execute_reply.started":"2025-05-20T05:27:55.273425Z","shell.execute_reply":"2025-05-20T07:08:49.748580Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Create sweep with ID: b84a7ohc\nSweep URL: https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9qn65dmr with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search_width: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlength_penalty: 0.6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adagrad\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_A3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_052802-9qn65dmr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/9qn65dmr' target=\"_blank\">confused-sweep-1</a></strong> to <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/9qn65dmr' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/9qn65dmr</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Seq2Seq(\n  (decoder): Decoder(\n    (dropout): Dropout(p=0.3, inplace=False)\n    (embedding): Embedding(29, 512)\n    (rnn): LSTM(512, 512, dropout=0.3, bidirectional=True)\n    (fc): Linear(in_features=1024, out_features=29, bias=True)\n    (log_softmax): LogSoftmax(dim=1)\n  )\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.3, inplace=False)\n    (embedding): Embedding(66, 512)\n    (rnn): LSTM(512, 512, dropout=0.3, bidirectional=True)\n  )\n)\nTotal Trainable Parameters: 8483357\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:30<00:00, 22.46it/s]\n100%|██████████| 69/69 [00:00<00:00, 89.92it/s]\n100%|██████████| 4358/4358 [00:31<00:00, 139.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 / 10 ===========================>\nTrain Accuracy Char: 53.3416, Train Average Loss: 1.5693\nValidation Accuracy Char: 56.9953, Validation Average Loss: 1.4006\nBeam Val Word Accuracy: 10.9224 Correct Prediction : 476/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:30<00:00, 22.43it/s]\n100%|██████████| 69/69 [00:00<00:00, 87.11it/s]\n100%|██████████| 4358/4358 [00:32<00:00, 135.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 / 10 ===========================>\nTrain Accuracy Char: 67.2685, Train Average Loss: 1.0879\nValidation Accuracy Char: 67.4658, Validation Average Loss: 1.0784\nBeam Val Word Accuracy: 23.4052 Correct Prediction : 1020/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:30<00:00, 22.50it/s]\n100%|██████████| 69/69 [00:00<00:00, 86.36it/s]\n100%|██████████| 4358/4358 [00:32<00:00, 132.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 / 10 ===========================>\nTrain Accuracy Char: 71.3614, Train Average Loss: 0.9563\nValidation Accuracy Char: 70.7289, Validation Average Loss: 0.9732\nBeam Val Word Accuracy: 29.0959 Correct Prediction : 1268/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:30<00:00, 22.42it/s]\n100%|██████████| 69/69 [00:00<00:00, 88.92it/s]\n100%|██████████| 4358/4358 [00:32<00:00, 133.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 / 10 ===========================>\nTrain Accuracy Char: 73.3290, Train Average Loss: 0.8896\nValidation Accuracy Char: 72.2835, Validation Average Loss: 0.9242\nBeam Val Word Accuracy: 32.5838 Correct Prediction : 1420/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:30<00:00, 22.45it/s]\n100%|██████████| 69/69 [00:00<00:00, 83.90it/s]\n100%|██████████| 4358/4358 [00:32<00:00, 132.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 / 10 ===========================>\nTrain Accuracy Char: 74.6330, Train Average Loss: 0.8447\nValidation Accuracy Char: 73.4423, Validation Average Loss: 0.8874\nBeam Val Word Accuracy: 35.6586 Correct Prediction : 1554/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:30<00:00, 22.39it/s]\n100%|██████████| 69/69 [00:00<00:00, 88.74it/s]\n100%|██████████| 4358/4358 [00:33<00:00, 131.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 / 10 ===========================>\nTrain Accuracy Char: 75.5525, Train Average Loss: 0.8131\nValidation Accuracy Char: 74.6987, Validation Average Loss: 0.8417\nBeam Val Word Accuracy: 36.9206 Correct Prediction : 1609/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:30<00:00, 22.49it/s]\n100%|██████████| 69/69 [00:00<00:00, 86.78it/s]\n100%|██████████| 4358/4358 [00:32<00:00, 132.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 / 10 ===========================>\nTrain Accuracy Char: 76.2637, Train Average Loss: 0.7889\nValidation Accuracy Char: 74.9891, Validation Average Loss: 0.8289\nBeam Val Word Accuracy: 37.7467 Correct Prediction : 1645/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:30<00:00, 22.49it/s]\n100%|██████████| 69/69 [00:00<00:00, 88.56it/s]\n100%|██████████| 4358/4358 [00:33<00:00, 131.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 / 10 ===========================>\nTrain Accuracy Char: 76.9721, Train Average Loss: 0.7627\nValidation Accuracy Char: 75.5132, Validation Average Loss: 0.8118\nBeam Val Word Accuracy: 38.4810 Correct Prediction : 1677/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:30<00:00, 22.43it/s]\n100%|██████████| 69/69 [00:00<00:00, 88.97it/s]\n100%|██████████| 4358/4358 [00:33<00:00, 131.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 / 10 ===========================>\nTrain Accuracy Char: 77.3789, Train Average Loss: 0.7467\nValidation Accuracy Char: 75.1176, Validation Average Loss: 0.8260\nBeam Val Word Accuracy: 39.4218 Correct Prediction : 1718/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:30<00:00, 22.47it/s]\n100%|██████████| 69/69 [00:00<00:00, 88.13it/s]\n100%|██████████| 4358/4358 [00:33<00:00, 131.29it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10 / 10 ===========================>\nTrain Accuracy Char: 77.5636, Train Average Loss: 0.7408\nValidation Accuracy Char: 75.5441, Validation Average Loss: 0.8097\nBeam Val Word Accuracy: 39.8348 Correct Prediction : 1736/4358\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>beam_val_accuracy_word</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>train_accuracy_char</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy_char</td><td>▁▅▆▇▇█████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>39.83479</td></tr><tr><td>beam_val_accuracy_word</td><td>39.83479</td></tr><tr><td>train_accuracy_char</td><td>77.56356</td></tr><tr><td>train_loss</td><td>0.74083</td></tr><tr><td>val_accuracy_char</td><td>75.54408</td></tr><tr><td>val_loss</td><td>0.80967</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell_LSTM_bs_64_ep_10_op_adagrad_drop_0.3_bsw_1_emb_512_hs_512_elayer_1_dlayer_1</strong> at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/9qn65dmr' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/9qn65dmr</a><br> View project at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_052802-9qn65dmr/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ngyna1om with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search_width: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlength_penalty: 0.6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adagrad\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_A3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_053901-ngyna1om</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/ngyna1om' target=\"_blank\">zesty-sweep-2</a></strong> to <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/ngyna1om' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/ngyna1om</a>"},"metadata":{}},{"name":"stdout","text":"Seq2Seq(\n  (decoder): Decoder(\n    (dropout): Dropout(p=0.5, inplace=False)\n    (embedding): Embedding(29, 512)\n    (rnn): GRU(512, 512, num_layers=2, dropout=0.5, bidirectional=True)\n    (fc): Linear(in_features=1024, out_features=29, bias=True)\n    (log_softmax): LogSoftmax(dim=1)\n  )\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.5, inplace=False)\n    (embedding): Embedding(66, 512)\n    (rnn): GRU(512, 512, num_layers=2, dropout=0.5, bidirectional=True)\n  )\n)\nTotal Trainable Parameters: 15831581\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.04it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.51it/s]\n100%|██████████| 4358/4358 [00:35<00:00, 122.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 / 10 ===========================>\nTrain Accuracy Char: 48.1034, Train Average Loss: 1.7790\nValidation Accuracy Char: 51.2886, Validation Average Loss: 1.6422\nBeam Val Word Accuracy: 4.9793 Correct Prediction : 217/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.08it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.16it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 117.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 / 10 ===========================>\nTrain Accuracy Char: 65.7938, Train Average Loss: 1.1209\nValidation Accuracy Char: 63.3521, Validation Average Loss: 1.2033\nBeam Val Word Accuracy: 15.3970 Correct Prediction : 671/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.07it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.09it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 114.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 / 10 ===========================>\nTrain Accuracy Char: 71.3523, Train Average Loss: 0.9488\nValidation Accuracy Char: 69.7988, Validation Average Loss: 1.0062\nBeam Val Word Accuracy: 26.7095 Correct Prediction : 1164/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.08it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.41it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 114.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 / 10 ===========================>\nTrain Accuracy Char: 73.8887, Train Average Loss: 0.8678\nValidation Accuracy Char: 72.4839, Validation Average Loss: 0.9204\nBeam Val Word Accuracy: 32.6985 Correct Prediction : 1425/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.10it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.15it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 113.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 / 10 ===========================>\nTrain Accuracy Char: 74.7742, Train Average Loss: 0.8354\nValidation Accuracy Char: 73.8482, Validation Average Loss: 0.8742\nBeam Val Word Accuracy: 36.1175 Correct Prediction : 1574/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 15.00it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.22it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 113.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 / 10 ===========================>\nTrain Accuracy Char: 75.8355, Train Average Loss: 0.7987\nValidation Accuracy Char: 74.7347, Validation Average Loss: 0.8401\nBeam Val Word Accuracy: 37.8614 Correct Prediction : 1650/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 14.91it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.14it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 111.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 / 10 ===========================>\nTrain Accuracy Char: 76.5681, Train Average Loss: 0.7730\nValidation Accuracy Char: 75.1587, Validation Average Loss: 0.8259\nBeam Val Word Accuracy: 39.7201 Correct Prediction : 1731/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 14.99it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.19it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 112.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 / 10 ===========================>\nTrain Accuracy Char: 77.3191, Train Average Loss: 0.7480\nValidation Accuracy Char: 75.5646, Validation Average Loss: 0.8216\nBeam Val Word Accuracy: 40.8215 Correct Prediction : 1779/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.04it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.71it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 112.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 / 10 ===========================>\nTrain Accuracy Char: 77.6811, Train Average Loss: 0.7319\nValidation Accuracy Char: 76.5590, Validation Average Loss: 0.7828\nBeam Val Word Accuracy: 41.0968 Correct Prediction : 1791/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.04it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.65it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 112.61it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10 / 10 ===========================>\nTrain Accuracy Char: 77.8960, Train Average Loss: 0.7231\nValidation Accuracy Char: 76.2250, Validation Average Loss: 0.7917\nBeam Val Word Accuracy: 41.3034 Correct Prediction : 1800/4358\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>beam_val_accuracy_word</td><td>▁▃▅▆▇▇████</td></tr><tr><td>train_accuracy_char</td><td>▁▅▆▇▇█████</td></tr><tr><td>train_loss</td><td>█▄▂▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy_char</td><td>▁▄▆▇▇▇████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>41.30335</td></tr><tr><td>beam_val_accuracy_word</td><td>41.30335</td></tr><tr><td>train_accuracy_char</td><td>77.89603</td></tr><tr><td>train_loss</td><td>0.72307</td></tr><tr><td>val_accuracy_char</td><td>76.22498</td></tr><tr><td>val_loss</td><td>0.79167</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell_GRU_bs_64_ep_10_op_adagrad_drop_0.5_bsw_1_emb_512_hs_512_elayer_2_dlayer_2</strong> at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/ngyna1om' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/ngyna1om</a><br> View project at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_053901-ngyna1om/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: r2x53o28 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search_width: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlength_penalty: 0.6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adagrad\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_A3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_055332-r2x53o28</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/r2x53o28' target=\"_blank\">stilted-sweep-3</a></strong> to <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/r2x53o28' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/r2x53o28</a>"},"metadata":{}},{"name":"stdout","text":"Seq2Seq(\n  (decoder): Decoder(\n    (dropout): Dropout(p=0.3, inplace=False)\n    (embedding): Embedding(29, 512)\n    (rnn): GRU(512, 512, num_layers=2, dropout=0.3, bidirectional=True)\n    (fc): Linear(in_features=1024, out_features=29, bias=True)\n    (log_softmax): LogSoftmax(dim=1)\n  )\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.3, inplace=False)\n    (embedding): Embedding(66, 512)\n    (rnn): GRU(512, 512, num_layers=2, dropout=0.3, bidirectional=True)\n  )\n)\nTotal Trainable Parameters: 15831581\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.05it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.46it/s]\n100%|██████████| 4358/4358 [00:36<00:00, 119.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 / 10 ===========================>\nTrain Accuracy Char: 49.6888, Train Average Loss: 1.7096\nValidation Accuracy Char: 52.3189, Validation Average Loss: 1.6081\nBeam Val Word Accuracy: 5.4153 Correct Prediction : 236/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 14.98it/s]\n100%|██████████| 69/69 [00:01<00:00, 53.21it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 117.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 / 10 ===========================>\nTrain Accuracy Char: 66.7818, Train Average Loss: 1.0855\nValidation Accuracy Char: 63.2005, Validation Average Loss: 1.2007\nBeam Val Word Accuracy: 15.7412 Correct Prediction : 686/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 14.93it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.11it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 113.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 / 10 ===========================>\nTrain Accuracy Char: 72.5662, Train Average Loss: 0.9086\nValidation Accuracy Char: 69.1462, Validation Average Loss: 1.0270\nBeam Val Word Accuracy: 25.7228 Correct Prediction : 1121/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 14.88it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.56it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 111.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 / 10 ===========================>\nTrain Accuracy Char: 75.0774, Train Average Loss: 0.8262\nValidation Accuracy Char: 72.3888, Validation Average Loss: 0.9163\nBeam Val Word Accuracy: 31.7577 Correct Prediction : 1384/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 14.92it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.16it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 112.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 / 10 ===========================>\nTrain Accuracy Char: 76.6498, Train Average Loss: 0.7733\nValidation Accuracy Char: 73.9690, Validation Average Loss: 0.8629\nBeam Val Word Accuracy: 36.0028 Correct Prediction : 1569/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 14.95it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.81it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 112.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 / 10 ===========================>\nTrain Accuracy Char: 77.5793, Train Average Loss: 0.7391\nValidation Accuracy Char: 75.0276, Validation Average Loss: 0.8363\nBeam Val Word Accuracy: 38.8251 Correct Prediction : 1692/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 14.92it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.88it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 113.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 / 10 ===========================>\nTrain Accuracy Char: 78.3711, Train Average Loss: 0.7121\nValidation Accuracy Char: 75.0585, Validation Average Loss: 0.8395\nBeam Val Word Accuracy: 39.8577 Correct Prediction : 1737/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 14.92it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.48it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 112.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 / 10 ===========================>\nTrain Accuracy Char: 78.8231, Train Average Loss: 0.6922\nValidation Accuracy Char: 76.0323, Validation Average Loss: 0.8080\nBeam Val Word Accuracy: 40.9592 Correct Prediction : 1785/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 14.94it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.25it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 111.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 / 10 ===========================>\nTrain Accuracy Char: 79.3329, Train Average Loss: 0.6738\nValidation Accuracy Char: 76.2841, Validation Average Loss: 0.7958\nBeam Val Word Accuracy: 41.2345 Correct Prediction : 1797/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 14.93it/s]\n100%|██████████| 69/69 [00:01<00:00, 50.69it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 111.58it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10 / 10 ===========================>\nTrain Accuracy Char: 79.5896, Train Average Loss: 0.6621\nValidation Accuracy Char: 76.1916, Validation Average Loss: 0.8066\nBeam Val Word Accuracy: 42.0376 Correct Prediction : 1832/4358\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>beam_val_accuracy_word</td><td>▁▃▅▆▇▇████</td></tr><tr><td>train_accuracy_char</td><td>▁▅▆▇▇█████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy_char</td><td>▁▄▆▇▇█████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>42.03763</td></tr><tr><td>beam_val_accuracy_word</td><td>42.03763</td></tr><tr><td>train_accuracy_char</td><td>79.58962</td></tr><tr><td>train_loss</td><td>0.6621</td></tr><tr><td>val_accuracy_char</td><td>76.19158</td></tr><tr><td>val_loss</td><td>0.80656</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell_GRU_bs_64_ep_10_op_adagrad_drop_0.3_bsw_1_emb_512_hs_512_elayer_2_dlayer_2</strong> at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/r2x53o28' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/r2x53o28</a><br> View project at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_055332-r2x53o28/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qijqd0mu with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search_width: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlength_penalty: 0.6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adagrad\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_A3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_060809-qijqd0mu</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/qijqd0mu' target=\"_blank\">bright-sweep-4</a></strong> to <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/qijqd0mu' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/qijqd0mu</a>"},"metadata":{}},{"name":"stdout","text":"Seq2Seq(\n  (decoder): Decoder(\n    (dropout): Dropout(p=0.5, inplace=False)\n    (embedding): Embedding(29, 512)\n    (rnn): LSTM(512, 512, num_layers=2, dropout=0.5, bidirectional=True)\n    (fc): Linear(in_features=1024, out_features=29, bias=True)\n    (log_softmax): LogSoftmax(dim=1)\n  )\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.5, inplace=False)\n    (embedding): Embedding(66, 512)\n    (rnn): LSTM(512, 512, num_layers=2, dropout=0.5, bidirectional=True)\n  )\n)\nTotal Trainable Parameters: 21082653\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:53<00:00, 12.97it/s]\n100%|██████████| 69/69 [00:01<00:00, 47.07it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 112.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 / 10 ===========================>\nTrain Accuracy Char: 46.1420, Train Average Loss: 1.8464\nValidation Accuracy Char: 45.2196, Validation Average Loss: 1.8908\nBeam Val Word Accuracy: 1.6292 Correct Prediction : 71/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:53<00:00, 12.99it/s]\n100%|██████████| 69/69 [00:01<00:00, 47.21it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 112.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 / 10 ===========================>\nTrain Accuracy Char: 63.8865, Train Average Loss: 1.1824\nValidation Accuracy Char: 61.3351, Validation Average Loss: 1.2839\nBeam Val Word Accuracy: 14.7545 Correct Prediction : 643/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:53<00:00, 13.02it/s]\n100%|██████████| 69/69 [00:01<00:00, 46.20it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 111.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 / 10 ===========================>\nTrain Accuracy Char: 69.8720, Train Average Loss: 0.9932\nValidation Accuracy Char: 68.7505, Validation Average Loss: 1.0371\nBeam Val Word Accuracy: 26.0441 Correct Prediction : 1135/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.14it/s]\n100%|██████████| 69/69 [00:01<00:00, 47.12it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 111.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 / 10 ===========================>\nTrain Accuracy Char: 72.7312, Train Average Loss: 0.9028\nValidation Accuracy Char: 71.3842, Validation Average Loss: 0.9494\nBeam Val Word Accuracy: 31.2529 Correct Prediction : 1362/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.16it/s]\n100%|██████████| 69/69 [00:01<00:00, 45.64it/s]\n100%|██████████| 4358/4358 [00:40<00:00, 108.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 / 10 ===========================>\nTrain Accuracy Char: 74.1299, Train Average Loss: 0.8539\nValidation Accuracy Char: 73.3832, Validation Average Loss: 0.8835\nBeam Val Word Accuracy: 35.0849 Correct Prediction : 1529/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.09it/s]\n100%|██████████| 69/69 [00:01<00:00, 47.01it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 109.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 / 10 ===========================>\nTrain Accuracy Char: 75.5048, Train Average Loss: 0.8093\nValidation Accuracy Char: 74.9146, Validation Average Loss: 0.8374\nBeam Val Word Accuracy: 37.3336 Correct Prediction : 1627/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.08it/s]\n100%|██████████| 69/69 [00:01<00:00, 46.64it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 108.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 / 10 ===========================>\nTrain Accuracy Char: 76.1535, Train Average Loss: 0.7838\nValidation Accuracy Char: 75.5929, Validation Average Loss: 0.8075\nBeam Val Word Accuracy: 38.2285 Correct Prediction : 1666/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.12it/s]\n100%|██████████| 69/69 [00:01<00:00, 46.19it/s]\n100%|██████████| 4358/4358 [00:40<00:00, 108.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 / 10 ===========================>\nTrain Accuracy Char: 76.6854, Train Average Loss: 0.7660\nValidation Accuracy Char: 75.1458, Validation Average Loss: 0.8194\nBeam Val Word Accuracy: 38.5498 Correct Prediction : 1680/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.15it/s]\n100%|██████████| 69/69 [00:01<00:00, 46.94it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 109.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 / 10 ===========================>\nTrain Accuracy Char: 77.1428, Train Average Loss: 0.7482\nValidation Accuracy Char: 75.4721, Validation Average Loss: 0.8128\nBeam Val Word Accuracy: 39.3759 Correct Prediction : 1716/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.15it/s]\n100%|██████████| 69/69 [00:01<00:00, 47.03it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 109.88it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10 / 10 ===========================>\nTrain Accuracy Char: 77.6194, Train Average Loss: 0.7343\nValidation Accuracy Char: 75.7316, Validation Average Loss: 0.7965\nBeam Val Word Accuracy: 39.6512 Correct Prediction : 1728/4358\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>beam_val_accuracy_word</td><td>▁▃▅▆▇█████</td></tr><tr><td>train_accuracy_char</td><td>▁▅▆▇▇█████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy_char</td><td>▁▅▆▇▇█████</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>39.65122</td></tr><tr><td>beam_val_accuracy_word</td><td>39.65122</td></tr><tr><td>train_accuracy_char</td><td>77.61938</td></tr><tr><td>train_loss</td><td>0.73425</td></tr><tr><td>val_accuracy_char</td><td>75.73165</td></tr><tr><td>val_loss</td><td>0.79653</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell_LSTM_bs_64_ep_10_op_adagrad_drop_0.5_bsw_1_emb_512_hs_512_elayer_2_dlayer_2</strong> at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/qijqd0mu' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/qijqd0mu</a><br> View project at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_060809-qijqd0mu/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u9kq6ooe with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search_width: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlength_penalty: 0.6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adagrad\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_A3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_062407-u9kq6ooe</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/u9kq6ooe' target=\"_blank\">wild-sweep-5</a></strong> to <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/u9kq6ooe' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/u9kq6ooe</a>"},"metadata":{}},{"name":"stdout","text":"Seq2Seq(\n  (decoder): Decoder(\n    (dropout): Dropout(p=0.3, inplace=False)\n    (embedding): Embedding(29, 512)\n    (rnn): LSTM(512, 512, num_layers=2, dropout=0.3, bidirectional=True)\n    (fc): Linear(in_features=1024, out_features=29, bias=True)\n    (log_softmax): LogSoftmax(dim=1)\n  )\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.3, inplace=False)\n    (embedding): Embedding(66, 512)\n    (rnn): LSTM(512, 512, num_layers=2, dropout=0.3, bidirectional=True)\n  )\n)\nTotal Trainable Parameters: 21082653\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.10it/s]\n100%|██████████| 69/69 [00:01<00:00, 47.88it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 117.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 / 10 ===========================>\nTrain Accuracy Char: 44.7516, Train Average Loss: 1.9027\nValidation Accuracy Char: 45.2941, Validation Average Loss: 1.8292\nBeam Val Word Accuracy: 2.2717 Correct Prediction : 99/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.12it/s]\n100%|██████████| 69/69 [00:01<00:00, 46.17it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 113.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 / 10 ===========================>\nTrain Accuracy Char: 64.3913, Train Average Loss: 1.1673\nValidation Accuracy Char: 61.3556, Validation Average Loss: 1.2594\nBeam Val Word Accuracy: 14.9610 Correct Prediction : 652/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.12it/s]\n100%|██████████| 69/69 [00:01<00:00, 46.36it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 111.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 / 10 ===========================>\nTrain Accuracy Char: 71.0614, Train Average Loss: 0.9570\nValidation Accuracy Char: 69.7731, Validation Average Loss: 0.9963\nBeam Val Word Accuracy: 26.6636 Correct Prediction : 1162/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.14it/s]\n100%|██████████| 69/69 [00:01<00:00, 46.03it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 110.62it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 / 10 ===========================>\nTrain Accuracy Char: 73.9199, Train Average Loss: 0.8640\nValidation Accuracy Char: 72.3271, Validation Average Loss: 0.9169\nBeam Val Word Accuracy: 32.1478 Correct Prediction : 1401/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.12it/s]\n100%|██████████| 69/69 [00:01<00:00, 46.34it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 110.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 / 10 ===========================>\nTrain Accuracy Char: 75.4642, Train Average Loss: 0.8094\nValidation Accuracy Char: 73.8919, Validation Average Loss: 0.8702\nBeam Val Word Accuracy: 35.8192 Correct Prediction : 1561/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.11it/s]\n100%|██████████| 69/69 [00:01<00:00, 46.36it/s]\n100%|██████████| 4358/4358 [00:40<00:00, 108.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 / 10 ===========================>\nTrain Accuracy Char: 76.5590, Train Average Loss: 0.7736\nValidation Accuracy Char: 75.2178, Validation Average Loss: 0.8263\nBeam Val Word Accuracy: 37.6319 Correct Prediction : 1640/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.11it/s]\n100%|██████████| 69/69 [00:01<00:00, 46.03it/s]\n100%|██████████| 4358/4358 [00:40<00:00, 108.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 / 10 ===========================>\nTrain Accuracy Char: 76.9803, Train Average Loss: 0.7578\nValidation Accuracy Char: 75.4259, Validation Average Loss: 0.8182\nBeam Val Word Accuracy: 39.4676 Correct Prediction : 1720/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.12it/s]\n100%|██████████| 69/69 [00:01<00:00, 47.24it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 110.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 / 10 ===========================>\nTrain Accuracy Char: 77.9032, Train Average Loss: 0.7241\nValidation Accuracy Char: 75.8113, Validation Average Loss: 0.8014\nBeam Val Word Accuracy: 39.8348 Correct Prediction : 1736/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.04it/s]\n100%|██████████| 69/69 [00:01<00:00, 46.25it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 109.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 / 10 ===========================>\nTrain Accuracy Char: 78.3384, Train Average Loss: 0.7080\nValidation Accuracy Char: 76.1864, Validation Average Loss: 0.7950\nBeam Val Word Accuracy: 40.5920 Correct Prediction : 1769/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:52<00:00, 13.08it/s]\n100%|██████████| 69/69 [00:01<00:00, 47.87it/s]\n100%|██████████| 4358/4358 [00:39<00:00, 109.74it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10 / 10 ===========================>\nTrain Accuracy Char: 78.8762, Train Average Loss: 0.6880\nValidation Accuracy Char: 76.1864, Validation Average Loss: 0.8018\nBeam Val Word Accuracy: 41.3263 Correct Prediction : 1801/4358\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>beam_val_accuracy_word</td><td>▁▃▅▆▇▇████</td></tr><tr><td>train_accuracy_char</td><td>▁▅▆▇▇█████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy_char</td><td>▁▅▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>41.3263</td></tr><tr><td>beam_val_accuracy_word</td><td>41.3263</td></tr><tr><td>train_accuracy_char</td><td>78.87624</td></tr><tr><td>train_loss</td><td>0.68801</td></tr><tr><td>val_accuracy_char</td><td>76.18644</td></tr><tr><td>val_loss</td><td>0.80185</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell_LSTM_bs_64_ep_10_op_adagrad_drop_0.3_bsw_1_emb_512_hs_512_elayer_2_dlayer_2</strong> at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/u9kq6ooe' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/u9kq6ooe</a><br> View project at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_062407-u9kq6ooe/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7zasksf6 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search_width: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlength_penalty: 0.6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adagrad\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_A3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_063959-7zasksf6</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/7zasksf6' target=\"_blank\">smooth-sweep-6</a></strong> to <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/7zasksf6' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/7zasksf6</a>"},"metadata":{}},{"name":"stdout","text":"Seq2Seq(\n  (decoder): Decoder(\n    (dropout): Dropout(p=0.5, inplace=False)\n    (embedding): Embedding(29, 512)\n    (rnn): GRU(512, 512, num_layers=2, dropout=0.5, bidirectional=True)\n    (fc): Linear(in_features=1024, out_features=29, bias=True)\n    (log_softmax): LogSoftmax(dim=1)\n  )\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.5, inplace=False)\n    (embedding): Embedding(66, 512)\n    (rnn): GRU(512, 512, num_layers=2, dropout=0.5, bidirectional=True)\n  )\n)\nTotal Trainable Parameters: 15831581\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 15.02it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.08it/s]\n100%|██████████| 4358/4358 [00:35<00:00, 121.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 / 10 ===========================>\nTrain Accuracy Char: 50.7071, Train Average Loss: 1.6686\nValidation Accuracy Char: 50.0475, Validation Average Loss: 1.6877\nBeam Val Word Accuracy: 4.3369 Correct Prediction : 189/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 15.02it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.81it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 117.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 / 10 ===========================>\nTrain Accuracy Char: 65.9465, Train Average Loss: 1.1126\nValidation Accuracy Char: 61.6177, Validation Average Loss: 1.2988\nBeam Val Word Accuracy: 13.8825 Correct Prediction : 605/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 15.02it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.82it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 115.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 / 10 ===========================>\nTrain Accuracy Char: 71.0359, Train Average Loss: 0.9556\nValidation Accuracy Char: 67.7304, Validation Average Loss: 1.0802\nBeam Val Word Accuracy: 22.0514 Correct Prediction : 961/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.16it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.31it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 114.67it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 / 10 ===========================>\nTrain Accuracy Char: 73.7827, Train Average Loss: 0.8696\nValidation Accuracy Char: 71.4484, Validation Average Loss: 0.9605\nBeam Val Word Accuracy: 29.8073 Correct Prediction : 1299/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.11it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.02it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 113.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 / 10 ===========================>\nTrain Accuracy Char: 75.1817, Train Average Loss: 0.8220\nValidation Accuracy Char: 73.0697, Validation Average Loss: 0.9063\nBeam Val Word Accuracy: 33.2951 Correct Prediction : 1451/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 14.99it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.66it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 113.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 / 10 ===========================>\nTrain Accuracy Char: 75.8734, Train Average Loss: 0.7975\nValidation Accuracy Char: 73.6761, Validation Average Loss: 0.8818\nBeam Val Word Accuracy: 36.4846 Correct Prediction : 1590/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 14.93it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.10it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 113.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 / 10 ===========================>\nTrain Accuracy Char: 76.5684, Train Average Loss: 0.7735\nValidation Accuracy Char: 74.8889, Validation Average Loss: 0.8397\nBeam Val Word Accuracy: 38.2515 Correct Prediction : 1667/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 15.00it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.68it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 113.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 / 10 ===========================>\nTrain Accuracy Char: 77.0491, Train Average Loss: 0.7551\nValidation Accuracy Char: 75.3154, Validation Average Loss: 0.8364\nBeam Val Word Accuracy: 39.5365 Correct Prediction : 1723/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.15it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.14it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 113.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 / 10 ===========================>\nTrain Accuracy Char: 77.4996, Train Average Loss: 0.7386\nValidation Accuracy Char: 74.9531, Validation Average Loss: 0.8516\nBeam Val Word Accuracy: 40.7067 Correct Prediction : 1774/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.14it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.69it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 112.95it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10 / 10 ===========================>\nTrain Accuracy Char: 77.8589, Train Average Loss: 0.7261\nValidation Accuracy Char: 76.1582, Validation Average Loss: 0.8005\nBeam Val Word Accuracy: 41.0509 Correct Prediction : 1789/4358\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>beam_val_accuracy_word</td><td>▁▃▄▆▇▇▇███</td></tr><tr><td>train_accuracy_char</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_accuracy_char</td><td>▁▄▆▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>41.05094</td></tr><tr><td>beam_val_accuracy_word</td><td>41.05094</td></tr><tr><td>train_accuracy_char</td><td>77.8589</td></tr><tr><td>train_loss</td><td>0.72612</td></tr><tr><td>val_accuracy_char</td><td>76.15817</td></tr><tr><td>val_loss</td><td>0.80055</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell_GRU_bs_64_ep_10_op_adagrad_drop_0.5_bsw_1_emb_512_hs_512_elayer_2_dlayer_2</strong> at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/7zasksf6' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/7zasksf6</a><br> View project at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_063959-7zasksf6/logs</code>"},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 02076rho with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_search_width: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: GRU\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlength_penalty: 0.6\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adagrad\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing: 0.7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'DA6401_A3' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_065429-02076rho</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/02076rho' target=\"_blank\">upbeat-sweep-7</a></strong> to <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/sweeps/b84a7ohc</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/02076rho' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/02076rho</a>"},"metadata":{}},{"name":"stdout","text":"Seq2Seq(\n  (decoder): Decoder(\n    (dropout): Dropout(p=0.3, inplace=False)\n    (embedding): Embedding(29, 512)\n    (rnn): GRU(512, 512, num_layers=2, dropout=0.3, bidirectional=True)\n    (fc): Linear(in_features=1024, out_features=29, bias=True)\n    (log_softmax): LogSoftmax(dim=1)\n  )\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.3, inplace=False)\n    (embedding): Embedding(66, 512)\n    (rnn): GRU(512, 512, num_layers=2, dropout=0.3, bidirectional=True)\n  )\n)\nTotal Trainable Parameters: 15831581\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.15it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.62it/s]\n100%|██████████| 4358/4358 [00:35<00:00, 121.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 / 10 ===========================>\nTrain Accuracy Char: 49.0389, Train Average Loss: 1.7459\nValidation Accuracy Char: 50.2865, Validation Average Loss: 1.6804\nBeam Val Word Accuracy: 4.5893 Correct Prediction : 200/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.16it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.57it/s]\n100%|██████████| 4358/4358 [00:36<00:00, 118.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 / 10 ===========================>\nTrain Accuracy Char: 66.1535, Train Average Loss: 1.1072\nValidation Accuracy Char: 62.4965, Validation Average Loss: 1.2379\nBeam Val Word Accuracy: 13.5154 Correct Prediction : 589/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.10it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.74it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 115.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 / 10 ===========================>\nTrain Accuracy Char: 72.2937, Train Average Loss: 0.9190\nValidation Accuracy Char: 68.9766, Validation Average Loss: 1.0253\nBeam Val Word Accuracy: 24.0018 Correct Prediction : 1046/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.10it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.16it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 114.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 / 10 ===========================>\nTrain Accuracy Char: 75.1669, Train Average Loss: 0.8248\nValidation Accuracy Char: 71.9289, Validation Average Loss: 0.9321\nBeam Val Word Accuracy: 30.7939 Correct Prediction : 1342/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.14it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.06it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 113.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 / 10 ===========================>\nTrain Accuracy Char: 76.5192, Train Average Loss: 0.7791\nValidation Accuracy Char: 74.2337, Validation Average Loss: 0.8564\nBeam Val Word Accuracy: 35.8880 Correct Prediction : 1564/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.13it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.28it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 113.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 / 10 ===========================>\nTrain Accuracy Char: 77.4959, Train Average Loss: 0.7437\nValidation Accuracy Char: 74.6268, Validation Average Loss: 0.8633\nBeam Val Word Accuracy: 38.8022 Correct Prediction : 1691/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.13it/s]\n100%|██████████| 69/69 [00:01<00:00, 53.06it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 112.28it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 / 10 ===========================>\nTrain Accuracy Char: 78.2659, Train Average Loss: 0.7158\nValidation Accuracy Char: 75.0636, Validation Average Loss: 0.8452\nBeam Val Word Accuracy: 40.2478 Correct Prediction : 1754/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.08it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.61it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 112.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 / 10 ===========================>\nTrain Accuracy Char: 78.4786, Train Average Loss: 0.7041\nValidation Accuracy Char: 76.3509, Validation Average Loss: 0.7937\nBeam Val Word Accuracy: 41.0968 Correct Prediction : 1791/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.06it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.98it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 112.81it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 / 10 ===========================>\nTrain Accuracy Char: 79.0794, Train Average Loss: 0.6819\nValidation Accuracy Char: 76.1736, Validation Average Loss: 0.8023\nBeam Val Word Accuracy: 41.4640 Correct Prediction : 1807/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:46<00:00, 15.02it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.78it/s]\n100%|██████████| 4358/4358 [00:38<00:00, 112.63it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 10 / 10 ===========================>\nTrain Accuracy Char: 79.4839, Train Average Loss: 0.6674\nValidation Accuracy Char: 76.5025, Validation Average Loss: 0.7873\nBeam Val Word Accuracy: 42.1524 Correct Prediction : 1837/4358\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>▁</td></tr><tr><td>beam_val_accuracy_word</td><td>▁▃▅▆▇▇████</td></tr><tr><td>train_accuracy_char</td><td>▁▅▆▇▇█████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy_char</td><td>▁▄▆▇▇▇████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>42.15236</td></tr><tr><td>beam_val_accuracy_word</td><td>42.15236</td></tr><tr><td>train_accuracy_char</td><td>79.48388</td></tr><tr><td>train_loss</td><td>0.66744</td></tr><tr><td>val_accuracy_char</td><td>76.50248</td></tr><tr><td>val_loss</td><td>0.78726</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cell_GRU_bs_64_ep_10_op_adagrad_drop_0.3_bsw_1_emb_512_hs_512_elayer_2_dlayer_2</strong> at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/02076rho' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/02076rho</a><br> View project at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_065429-02076rho/logs</code>"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"\n# Load Dataset\n# df_train, train_input_len, train_out_len = load_dataset('/kaggle/input/dakshina-dataset-ass-3/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv')\n# df_val, val_input_len, val_out_len = load_dataset('/kaggle/input/dakshina-dataset-ass-3/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv')\n# df_test, test_input_len, test_out_len = load_dataset('/kaggle/input/dakshina-dataset-ass-3/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv')\n\ndf_train, train_input_len, train_out_len = load_dataset('/kaggle/input/dakshina/hi.translit.sampled.train.tsv')\ndf_val, val_input_len, val_out_len = load_dataset('/kaggle/input/dakshina/hi.translit.sampled.dev.tsv')\ndf_test, test_input_len, test_out_len = load_dataset('/kaggle/input/dakshina/hi.translit.sampled.test.tsv')\n\n\ninput_max_len = max(train_input_len, val_input_len, test_input_len)\noutput_max_len = max(train_out_len, val_out_len, test_out_len)\n\nmax_length = max(input_max_len, output_max_len)\n\n# Create Look Up Table\ninput_char_to_int, input_int_to_char = look_up_table(df_train[0], df_val[0], df_test[0])\noutput_char_to_int, output_int_to_char = look_up_table(df_train[1], df_val[1], df_test[1])\n\n\n\nparams = {\n        \"input_size\": len(input_char_to_int),\n        \"output_size\": len(output_char_to_int),\n        \"embedding_size\": 512,\n        \"hidden_size\": 512,\n        \"enc_num_layers\": 2,\n        \"dec_num_layers\": 2,\n        \"cell_type\": \"GRU\", # LSTM, GRU, RNN\n        \"dropout\": 0.3,\n        \"learning_rate\": 0.01,\n        \"batch_size\": 64,\n        \"num_epochs\": 12,\n        \"optimizer\": 'adagrad',  # ['sgd', 'rmsprop', 'adam', 'nadam']\n        \"beam_search_width\" : 1,\n        \"length_penalty\" : 0.6,\n        \"bidirectional\": True,\n        \"teacher_forcing\":0.7,\n\n    }\n\n\n# Data Embedding and Converting them into Tensor\ntrain_inputs, train_outputs = get_tensor_object(df_train, max_length, max_length, input_char_to_int, output_char_to_int)\nval_inputs, val_outputs = get_tensor_object(df_val, max_length, max_length, input_char_to_int, output_char_to_int)\ntest_inputs, test_outputs = get_tensor_object(df_test, max_length, max_length, input_char_to_int, output_char_to_int)\n\n# Transpose column wise\ntrain_inputs, train_outputs = torch.transpose(train_inputs, 0, 1), torch.transpose(train_outputs, 0, 1)\nval_inputs, val_outputs = torch.transpose(val_inputs, 0, 1), torch.transpose(val_outputs, 0, 1)\ntest_inputs, test_outputs = torch.transpose(test_inputs, 0, 1), torch.transpose(test_outputs, 0, 1)\n\n\n# Initialize Hyperparameters\ninput_size = params['input_size']\noutput_size = params['output_size']\nembedding_size = params['embedding_size']\nhidden_size = params['hidden_size']\nenc_num_layers = params['enc_num_layers'] \ndec_num_layers = params['dec_num_layers']  \ncell_type = params['cell_type']\ndropout = params['dropout']\nlearning_rate = params['learning_rate']\nbatch_size = params['batch_size']\nnum_epochs = params['num_epochs']  \noptimizer = params['optimizer']  \nbeam_width = params['beam_search_width']\nbidirectional = params['bidirectional']\nlength_penalty = params['length_penalty']\nteacher_forcing = params['teacher_forcing']\n\n# Create train data batch\ntrain_batch_x, train_batch_y = torch.split(train_inputs, batch_size, dim=1), torch.split(train_outputs, batch_size, dim=1)\n# Validation data batch\nval_batch_x, val_batch_y = torch.split(val_inputs, batch_size, dim=1), torch.split(val_outputs, batch_size, dim=1)\n\n\n# Intialize encoder, decoder and seq2seq model\nencoder = Encoder(input_size, embedding_size, hidden_size, enc_num_layers, dropout, bidirectional, cell_type).to(device)\ndecoder = Decoder(output_size, embedding_size, hidden_size, output_size, dec_num_layers, dropout, bidirectional, cell_type).to(device)  \nmodel = Seq2Seq(encoder, decoder, output_char_to_int, teacher_forcing, cell_type).to(device)\n\n# Print total number of parameters in the model\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(model)\nprint(f'Total Trainable Parameters: {total_params}')\n\n\n# Loss function and Optimizer\ncriterion = nn.CrossEntropyLoss()\nif optimizer == 'adam':\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\nelif optimizer == 'sgd':\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\nelif optimizer == 'rmsprop':\n    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\nelif optimizer == 'nadam':\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\nelif optimizer == 'adagrad':\n    optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\nelse:\n    print(\"Incorrect Optmizer !!!!\")\n\n# TRAINING\nmodel, acc = train(model, num_epochs, criterion, optimizer, train_batch_x, train_batch_y, val_batch_x, val_batch_y, df_val, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, cell_type, max_length, 0)","metadata":{"execution":{"iopub.status.busy":"2025-05-20T07:26:24.011786Z","iopub.execute_input":"2025-05-20T07:26:24.012364Z","iopub.status.idle":"2025-05-20T07:43:14.734695Z","shell.execute_reply.started":"2025-05-20T07:26:24.012339Z","shell.execute_reply":"2025-05-20T07:43:14.734047Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Seq2Seq(\n  (decoder): Decoder(\n    (dropout): Dropout(p=0.3, inplace=False)\n    (embedding): Embedding(29, 512)\n    (rnn): GRU(512, 512, num_layers=2, dropout=0.3, bidirectional=True)\n    (fc): Linear(in_features=1024, out_features=29, bias=True)\n    (log_softmax): LogSoftmax(dim=1)\n  )\n  (encoder): Encoder(\n    (dropout): Dropout(p=0.3, inplace=False)\n    (embedding): Embedding(66, 512)\n    (rnn): GRU(512, 512, num_layers=2, dropout=0.3, bidirectional=True)\n  )\n)\nTotal Trainable Parameters: 15831581\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.10it/s]\n100%|██████████| 69/69 [00:01<00:00, 51.96it/s]\n100%|██████████| 4358/4358 [00:34<00:00, 125.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 / 12 ===========================>\nTrain Accuracy Char: 49.3185, Train Average Loss: 1.7432\nValidation Accuracy Char: 50.6025, Validation Average Loss: 1.6916\nBeam Val Word Accuracy: 3.9927 Correct Prediction : 174/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.32it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.66it/s]\n100%|██████████| 4358/4358 [00:35<00:00, 121.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 / 12 ===========================>\nTrain Accuracy Char: 66.2777, Train Average Loss: 1.1011\nValidation Accuracy Char: 61.4892, Validation Average Loss: 1.2818\nBeam Val Word Accuracy: 13.1941 Correct Prediction : 575/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.28it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.41it/s]\n100%|██████████| 4358/4358 [00:36<00:00, 118.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 / 12 ===========================>\nTrain Accuracy Char: 72.2799, Train Average Loss: 0.9189\nValidation Accuracy Char: 68.7453, Validation Average Loss: 1.0342\nBeam Val Word Accuracy: 24.0018 Correct Prediction : 1046/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.27it/s]\n100%|██████████| 69/69 [00:01<00:00, 53.63it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 117.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 / 12 ===========================>\nTrain Accuracy Char: 75.0666, Train Average Loss: 0.8266\nValidation Accuracy Char: 71.6668, Validation Average Loss: 0.9441\nBeam Val Word Accuracy: 31.2758 Correct Prediction : 1363/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.18it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.92it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 116.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 / 12 ===========================>\nTrain Accuracy Char: 76.4056, Train Average Loss: 0.7810\nValidation Accuracy Char: 74.4598, Validation Average Loss: 0.8528\nBeam Val Word Accuracy: 35.7274 Correct Prediction : 1557/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.23it/s]\n100%|██████████| 69/69 [00:01<00:00, 53.39it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 116.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 / 12 ===========================>\nTrain Accuracy Char: 77.3789, Train Average Loss: 0.7451\nValidation Accuracy Char: 74.8092, Validation Average Loss: 0.8422\nBeam Val Word Accuracy: 37.7696 Correct Prediction : 1646/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.24it/s]\n100%|██████████| 69/69 [00:01<00:00, 53.50it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 116.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 / 12 ===========================>\nTrain Accuracy Char: 78.2010, Train Average Loss: 0.7177\nValidation Accuracy Char: 75.5492, Validation Average Loss: 0.8237\nBeam Val Word Accuracy: 40.0642 Correct Prediction : 1746/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.24it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.77it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 116.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 / 12 ===========================>\nTrain Accuracy Char: 78.8113, Train Average Loss: 0.6951\nValidation Accuracy Char: 75.8704, Validation Average Loss: 0.8155\nBeam Val Word Accuracy: 40.8903 Correct Prediction : 1782/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.24it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.58it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 116.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 / 12 ===========================>\nTrain Accuracy Char: 79.1204, Train Average Loss: 0.6818\nValidation Accuracy Char: 76.7928, Validation Average Loss: 0.7757\nBeam Val Word Accuracy: 41.2575 Correct Prediction : 1798/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.19it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.75it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 115.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 / 12 ===========================>\nTrain Accuracy Char: 79.5299, Train Average Loss: 0.6636\nValidation Accuracy Char: 76.1813, Validation Average Loss: 0.8031\nBeam Val Word Accuracy: 41.8311 Correct Prediction : 1823/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.17it/s]\n100%|██████████| 69/69 [00:01<00:00, 52.72it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 115.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 / 12 ===========================>\nTrain Accuracy Char: 79.8188, Train Average Loss: 0.6528\nValidation Accuracy Char: 76.7569, Validation Average Loss: 0.7785\nBeam Val Word Accuracy: 41.8770 Correct Prediction : 1825/4358\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 691/691 [00:45<00:00, 15.25it/s]\n100%|██████████| 69/69 [00:01<00:00, 53.21it/s]\n100%|██████████| 4358/4358 [00:37<00:00, 114.88it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 12 / 12 ===========================>\nTrain Accuracy Char: 80.0394, Train Average Loss: 0.6419\nValidation Accuracy Char: 76.8237, Validation Average Loss: 0.7791\nBeam Val Word Accuracy: 41.9458 Correct Prediction : 1828/4358\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## TEST PREDICTON","metadata":{}},{"cell_type":"code","source":"def store_results(data_type, words, translations, predictions, results):\n    \"\"\"\n    This function saves the evaluation results to a CSV file.\n\n    Args:\n        data_type (str): The type of data used for evaluation (e.g., 'val', 'test').\n        words (list): List of source words (without start/end tokens).\n        translations (list): List of reference translations (without start/end tokens).\n        predictions (list): List of predicted translated sequences (without start/end tokens).\n        results (list): List of 'Yes' or 'No' indicating correct/incorrect predictions.\n    \"\"\"\n\n    # Create a dictionary to store the results in a structured format\n    log = {\n        'Word': words,\n        'Translation': translations,\n        'Prediction': predictions,\n        'Result': results  # 'Yes' for correct, 'No' for incorrect\n    }\n    \n    # Construct the file path for the CSV file\n    path = '/kaggle/working/predictions.csv'\n\n    # Create a Pandas DataFrame from the dictionary\n    data_frame = pd.DataFrame(log)\n\n    # Save the DataFrame to a CSV file (header=True includes column names, index=False excludes row index)\n    data_frame.to_csv(path, header=True, index=False)\n    \n    # Log to wandb\n    wandb.init(project='DA6401_A3', name='Prediction_Store')\n\n    wandb.log({'Prediction_table': wandb.Table(dataframe= data_frame)})\n\n    wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:37:41.811884Z","iopub.execute_input":"2024-05-06T19:37:41.812615Z","iopub.status.idle":"2024-05-06T19:37:41.819622Z","shell.execute_reply.started":"2024-05-06T19:37:41.812581Z","shell.execute_reply":"2024-05-06T19:37:41.818686Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"\ntest_acc = 0\ncorrect_pred = 0\nwords_test = [] \ntranslations_test = [] \npredictions_test = []\nresults_test = []\n\nfor i in tqdm(range(df_test.shape[0])):\n    input_seq = df_test.iloc[i, 0][:-1] \n    true_seq = df_test.iloc[i, 1][1:-1]\n    predicted_output = beam_search(model, input_seq, max_length, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, cell_type)\n    words_test.append(input_seq)\n    translations_test.append(true_seq)\n    predictions_test.append(predicted_output[:-1])\n    if true_seq == predicted_output[:-1]:\n        correct_pred += 1\n        results_test.append('Yes')\n    else:\n        results_test.append('No')\n\ntest_acc = 100 * correct_pred / df_test.shape[0]   \n\nprint(f'Test Accuracy Word Level: {test_acc}, Correctly Predicted: {correct_pred}')\nwandb.init(project='DA6401_A3', name='bestmodel_test')\n\nwandb.log({ \"val_accuracy_word\" : acc,\n            \"test_accuracy_word\" : test_acc\n            })\n\nwandb.finish()\n#store_results('test', words_test, translations_test, predictions_test, results_test)","metadata":{"execution":{"iopub.status.busy":"2025-05-20T07:49:16.260352Z","iopub.execute_input":"2025-05-20T07:49:16.260685Z","iopub.status.idle":"2025-05-20T07:50:03.365542Z","shell.execute_reply.started":"2025-05-20T07:49:16.260660Z","shell.execute_reply":"2025-05-20T07:50:03.364974Z"},"trusted":true},"outputs":[{"name":"stderr","text":"100%|██████████| 4502/4502 [00:39<00:00, 114.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Test Accuracy Word Level: 40.58196357174589, Correctly Predicted: 1827\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_074955-8xt40v2t</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/8xt40v2t' target=\"_blank\">bestmodel_test</a></strong> to <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/8xt40v2t' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/8xt40v2t</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy_word</td><td>▁</td></tr><tr><td>val_accuracy_word</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy_word</td><td>40.58196</td></tr><tr><td>val_accuracy_word</td><td>41.94585</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">bestmodel_test</strong> at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/8xt40v2t' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3/runs/8xt40v2t</a><br> View project at: <a href='https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3' target=\"_blank\">https://wandb.ai/cs24m030-indian-institute-of-technology-madras/DA6401_A3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250520_074955-8xt40v2t/logs</code>"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"# if __name__ == \"__main__\":\n#     parser.add_argument('-dp', '--data_path', type=str, default='kaggle/input/hinid-dataset/aksharantar_sampled/hin', help='Path to the data folder')\n#     parser.add_argument('-l', '--lang', type=str, default='hin', help='Language for which training is to be done')\n#     parser.add_argument('-es', '--embedding_size', type=int, default=256, help='Embedding size')\n#     parser.add_argument('-hs', '--hidden_size', type=int, default=512, help='Hidden size')\n#     parser.add_argument('-nl', '--num_layers', type=int, default=2, help='Number of layers')\n#     parser.add_argument('-ct', '--cell_type', type=str, default='LSTM', choices=['RNN', 'LSTM', 'GRU'], help='Cell type (RNN, LSTM, GRU)')\n#     parser.add_argument('-dr', '--dropout', type=float, default=0.3, help='Dropout rate')\n#     parser.add_argument('-lr', '--learning_rate', type=float, default=0.01, help='Learning rate')\n#     parser.add_argument('-bs', '--batch_size', type=int, default=32, help='Batch size')\n#     parser.add_argument('-ne', '--num_epochs', type=int, default=10, help='Number of epochs')\n#     parser.add_argument('-op', '--optimizer', type=str, default='adagrad', choices=['adam', 'sgd', 'rmsprop', 'nadam', 'adagrad'], help='Optimizer (adam, sgd, rmsprop, nadam, adagrad)')\n#     parser.add_argument('-bw', '--beam_search_width', type=int, default=1, help='Beam search width')\n#     parser.add_argument('-lp', '--length_penalty', type=float, default=0.6, help='Length penalty')\n#     parser.add_argument('-tf', '--teacher_forcing', type=float, default=0.7, help='Teacher forcing ratio')\n#     parser.add_argument('-bi', '--bidirectional', action='store_true', default=True, help='Use bidirectional encoder')\n#     parser.add_argument('--wandb_log', type=int, default=0, help='Whether to log to WandB (1 for yes, 0 for no)')\n    \n    \n#     config = parser.parse_args()\n#     data_path = config.data_path\n#     lang = config.lang\n    \n    \n#     # Load Dataset\n#     df_train, train_input_len, train_out_len = load_dataset(f'/{data_path}/{lang}/{lang}_train.csv')\n#     df_val, val_input_len, val_out_len = load_dataset(f'/{data_path}/{lang}/{lang}_valid.csv')\n#     df_test, test_input_len, test_out_len = load_dataset(f'/{data_path}/{lang}/{lang}_test.csv')\n\n#     input_max_len = max(train_input_len, val_input_len, test_input_len)\n#     output_max_len = max(train_out_len, val_out_len, test_out_len)\n    \n#     max_length = max(input_max_len, output_max_len)\n\n#     # Create Look Up Table\n#     input_char_to_int, input_int_to_char = look_up_table(df_train[0], df_val[0], df_test[0])\n#     output_char_to_int, output_int_to_char = look_up_table(df_train[1], df_val[1], df_test[1])\n\n#     # Data Embedding and Converting them into Tensor\n#     train_inputs, train_outputs = get_tensor_object(df_train, max_length, max_length, input_char_to_int, output_char_to_int)\n#     val_inputs, val_outputs = get_tensor_object(df_val, max_length, max_length, input_char_to_int, output_char_to_int)\n#     test_inputs, test_outputs = get_tensor_object(df_test, max_length, max_length, input_char_to_int, output_char_to_int)\n\n#     # Transpose column wise\n#     train_inputs, train_outputs = torch.transpose(train_inputs, 0, 1), torch.transpose(train_outputs, 0, 1)\n#     val_inputs, val_outputs = torch.transpose(val_inputs, 0, 1), torch.transpose(val_outputs, 0, 1)\n#     test_inputs, test_outputs = torch.transpose(test_inputs, 0, 1), torch.transpose(test_outputs, 0, 1)\n    \n#     # Initialize Hyperparameters\n#     input_size = len(input_char_to_int)\n#     output_size = len(output_char_to_int)\n#     embedding_size = config.embedding_size\n#     hidden_size = config.hidden_size\n#     enc_num_layers = config.num_layers\n#     dec_num_layers = config.num_layers\n#     cell_type = config.cell_type\n#     dropout = config.dropout\n#     learning_rate = config.learning_rate\n#     batch_size = config.batch_size\n#     num_epochs = config.num_epochs  \n#     optimizer = config.optimizer  \n#     beam_width = config.beam_search_width\n#     bidirectional = config.bidirectional\n#     length_penalty = config.length_penalty\n#     teacher_forcing = config.teacher_forcing\n#     learning_rate = config.learning_rate\n    \n#     # Create train data batch\n#     train_batch_x, train_batch_y = torch.split(train_inputs, batch_size, dim=1), torch.split(train_outputs, batch_size, dim=1)\n#     # Validation data batch\n#     val_batch_x, val_batch_y = torch.split(val_inputs, batch_size, dim=1), torch.split(val_outputs, batch_size, dim=1)\n\n\n#     # Intialize encoder, decoder and seq2seq model\n#     encoder = Encoder(input_size, embedding_size, hidden_size, enc_num_layers, dropout, bidirectional, cell_type).to(device)\n#     decoder = Decoder(output_size, embedding_size, hidden_size, output_size, dec_num_layers, dropout, bidirectional, cell_type).to(device)  \n#     model = Seq2Seq(encoder, decoder, output_char_to_int, teacher_forcing, cell_type).to(device)\n\n#     # Print total number of parameters in the model\n#     total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n#     print(model)\n#     print(f'Total Trainable Parameters: {total_params}')\n\n\n#     # Loss function and Optimizer\n#     criterion = nn.CrossEntropyLoss()\n#     if optimizer == 'adam':\n#         optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n#     elif optimizer == 'sgd':\n#         optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n#     elif optimizer == 'rmsprop':\n#         optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n#     elif optimizer == 'nadam':\n#         optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n#     elif optimizer == 'adagrad':\n#         optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n\n#     # TRAINING\n    \n#     if config.wandb_log == 1:  \n#         wandb.init(project='DA6401_A3')\n#         wandb.run.name = 'cell_' + config.cell_type + '_bs_' + str(config.batch_size) + '_ep_' + str(config.num_epochs) + '_op_' + str(config.optimizer) + '_drop_' + str(config.dropout) + '_bsw_' + str(config.beam_search_width) +'_emb_' + str(config.embedding_size) + '_hs_' + str(config.hidden_size) + '_elayer_' + str(config.num_layers) + '_dlayer_' + str(config.num_layers)\n\n#     model, acc = train(model, num_epochs, criterion, optimizer, train_batch_x, train_batch_y, val_batch_x, val_batch_y, df_val, input_char_to_int, output_char_to_int, output_int_to_char, beam_width, length_penalty, cell_type, max_length, config.wandb_log)\n#     if config.wandb_log == 1: \n#         wandb.log({\n#                 \"accuracy\": acc,\n#             })","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Example usage\n# for i in range(10):\n#     input_seq = df_train.iloc[i, 0][:-1] \n#     predicted_output = beam_search(model, input_seq, input_char_to_int, output_char_to_int, output_int_to_char, 1, 0.6, \"RNN\")\n\n#     print(f\"Input Sequence {i+1}: {input_seq}\")\n#     print(f\"Predicted Output Sequence {i+1}: {predicted_output}\\n\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}